{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYZrG2uquimp"
   },
   "outputs": [],
   "source": [
    "# An example of reading data from an .xml file with Python, using the \"lxml\"\n",
    "# library.\n",
    "# First, you'll need to pip install the lxml library:\n",
    "# https://pypi.org/project/lxml/\n",
    "# The data used here is an instance of\n",
    "# http://feeds.bbci.co.uk/news/science_and_environment/rss.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hJppXCkuim2"
   },
   "outputs": [],
   "source": [
    "# specify the \"chapter\" of the `lxml` library you want to import,\n",
    "# in this case, `etree`, which stands for \"ElementTree\"\n",
    "from lxml import etree\n",
    "\n",
    "# import the `csv` library, to create our output file\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca_KOljKuxo9"
   },
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
    "# # Import PyDrive and associated libraries.\n",
    "# # This only needs to be done once per notebook.\n",
    "# # Documentation found here: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# # This only needs to be done once per notebook.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Pnw6oJOuyCO"
   },
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
    "# # Link to data file stored in Drive: https://drive.google.com/file/d/185Mg-1rkd_hFzolqpeJ7wbignG-dQ-ql/view?usp=sharing\n",
    "# file_id = '185Mg-1rkd_hFzolqpeJ7wbignG-dQ-ql' # notice where this string comes from in link above\n",
    "\n",
    "# imported_file = drive.CreateFile({'id': file_id}) # creating an accessible copy of the shared data file\n",
    "# print(imported_file['title'])  # it should print the title of desired file\n",
    "# imported_file.GetContentFile(imported_file['title']) # refer to it in this notebook by the same name as it has in Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFIE4NRVuim4"
   },
   "outputs": [],
   "source": [
    "# choose a filename, for simplicity\n",
    "filename = \"BBC News - Science Environment XML Feed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnBs_zI1uim4"
   },
   "outputs": [],
   "source": [
    "# open our data file in read format, using \"rb\" as the \"mode\"\n",
    "xml_source_file = open(filename+\".xml\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79M766HHuim5"
   },
   "outputs": [],
   "source": [
    "# pass our xml_source_file as an ingredient to the the `lxml` library's\n",
    "# `etree.parse()` method and store the result in a variable called `xml_doc`\n",
    "xml_doc = etree.parse(xml_source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVhZ3-HLuim6"
   },
   "outputs": [],
   "source": [
    "# start by getting the current xml document's \"root\" element\n",
    "document_root = xml_doc.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzS735ZQuim7"
   },
   "outputs": [],
   "source": [
    "# if the document_root is a well-formed XML element\n",
    "if etree.iselement(document_root):\n",
    "\n",
    "    # create our output file, naming it \"rss_\"+filename+\".csv\"\n",
    "    output_file = open(\"rss_\"+filename+\".csv\",\"w\")\n",
    "\n",
    "    # use the `csv` library's \"writer\" recipe to easily write rows of data\n",
    "    # to `output_file`, instead of reading data *from* it\n",
    "    output_writer = csv.writer(output_file)\n",
    "\n",
    "    # document_root[0] is the \"channel\" element\n",
    "    main_channel = document_root[0]\n",
    "\n",
    "    # the `find()` method returns *only* the first instance of the element name\n",
    "    article_example = main_channel.find('item')\n",
    "\n",
    "    # create an empty list in which to store our future column headers\n",
    "    tag_list = []\n",
    "    for child in article_example.iterdescendants():\n",
    "\n",
    "        # add each tag to our would-be header list\n",
    "        tag_list.append(child.tag)\n",
    "\n",
    "        # if the current tag has any attributes\n",
    "        if child.attrib:\n",
    "\n",
    "            # loop through the attribute keys in the tag\n",
    "            for attribute_name in child.attrib.keys():\n",
    "\n",
    "                # append the attribute name to our `tag_list` column headers\n",
    "                tag_list.append(attribute_name)\n",
    "\n",
    "    # write the contents of `tag_list` to our output file as column headers\n",
    "    output_writer.writerow(tag_list)\n",
    "\n",
    "    # now we want to grab *every* <item> elment in our file\n",
    "    # so we use the `findall()` method instead of `find()`\n",
    "    for item in main_channel.findall('item'):\n",
    "\n",
    "        # empty list for holding our new row's content\n",
    "        new_row = []\n",
    "\n",
    "        # now we'll use our list of tags to get the contents of each element\n",
    "        for tag in tag_list:\n",
    "\n",
    "            # if there is anything in the element with a given tag name\n",
    "            if item.findtext(tag):\n",
    "\n",
    "                # append it to our new row\n",
    "                new_row.append(item.findtext(tag))\n",
    "\n",
    "            # otherwise, make sure it's the \"isPermaLink\" attribute\n",
    "            elif tag == \"isPermaLink\":\n",
    "\n",
    "                # grab its value from the <guid> element\n",
    "                # and append it to our row\n",
    "                new_row.append(item.find('guid').get(\"isPermaLink\"))\n",
    "\n",
    "        # write the new row to our output file!\n",
    "        output_writer.writerow(new_row)\n",
    "\n",
    "    # officially close the `.csv` file we just wrote all that data to\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6H1qy9hCukwd"
   },
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
    "# from google.colab import files\n",
    "\n",
    "# files.download(\"rss_\"+filename+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "rss_parsing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
